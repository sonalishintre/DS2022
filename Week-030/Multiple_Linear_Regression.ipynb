{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11c98c29",
   "metadata": {},
   "source": [
    "\n",
    "    In our previous understanding of Linear Regression, we were capable of handling only 1 input variable. We were using that one input variable to drive our regression line. This is not necessarily the ideal way to create a predictive model. In the real world, outputs can take on a range of features, 2,3,5,1000,1000000, the list is limitless. We have to adopt a new way of expression our update rule so that we can correctly handle more than 1 input variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f96e05",
   "metadata": {},
   "source": [
    "# Old Hypothesis\n",
    "\n",
    "$h_{\\theta}x = \\theta_{0} + \\theta_{1}x$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497a7bc2",
   "metadata": {},
   "source": [
    "This was the hypothesis generated.\n",
    "\n",
    "We would use this to drive what we were solving for. \n",
    "\n",
    "For MSE, we were looking to find the thetas that minimized the loss function\n",
    "\n",
    "$MSE = \\frac{1}{m}\\Sigma^{m}_{i=1}(h_{\\theta}x-y_{pred})^2$\n",
    "\n",
    "\n",
    "\n",
    "When we broke it down to its partial derivatives, we got these two update rules for theta_1 and theta_0\n",
    "\n",
    "\n",
    "$\\theta_{0} = \\theta_{0} - \\alpha\\frac{1}{m}\\Sigma^{m}_{i=1}(h_{\\theta}x-y_{pred})$\n",
    "\n",
    "$\\theta_{1} = \\theta_{1} - \\alpha\\frac{1}{m}\\Sigma^{m}_{i=1}(h_{\\theta}x-y_{pred})x$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b486fca",
   "metadata": {},
   "source": [
    "# New Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1137cef5",
   "metadata": {},
   "source": [
    "$h_{\\theta}x = \\theta_{0} + \\theta_{1}x^{1} + \\theta_{2}x^{2} + \\theta_{3}x^{3} + ... + \\theta_{n}x^{n}$\n",
    "\n",
    "\n",
    "A way of thinking about this, is that each of these vectors contribute some amount to the output value. The theta value that is assigned as the coefficient is how much they contribute to the output. \n",
    "\n",
    "We want to get used to using matrix notation, as this will vastly increase the speed of our program. We can simplify our hypothesis one step further.\n",
    "\n",
    "\n",
    "$h_{\\theta}x = \\theta^{\\intercal}x$\n",
    "\n",
    "\n",
    "We want to use this, but it's not entirely usable in its current format. Remember our rules for Matrix Matrix multiplication. The columns of the first Matrix have to match the rows of the second Matrix.\n",
    "\n",
    "Right now, our theta vector has one more column than our X has rows. What can we do to skirt around this? We can add a feature to our data set\n",
    "$x_{0}$. This is going to be a vector of all 1s.\n",
    "\n",
    "Adding this in now let's us correctly simplify our hypothesis as stated previously."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c086d4c8",
   "metadata": {},
   "source": [
    "# Our new update rule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74d4340",
   "metadata": {},
   "source": [
    "\n",
    "$\\theta_{0} = \\theta_{0} - \\alpha\\frac{1}{m}\\Sigma^{m}_{i=1}(h_{\\theta}x-y_{pred})$\n",
    "\n",
    "$\\theta_{1} = \\theta_{1} - \\alpha\\frac{1}{m}\\Sigma^{m}_{i=1}(h_{\\theta}x-y_{pred})x$\n",
    "\n",
    "This was our previous update rule for gradient descent, it allowed us to iteratively find the gradient of the loss function, and then move towards the opposite direction of the gradient. This however only accounts for a singular feature. \n",
    "\n",
    "\n",
    "Our update rule now has to be able to handle all of the features.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Our new update rule is going to be as follows\n",
    "\n",
    "$\\theta_{j} = \\theta_{j} - \\alpha\\frac{1}{m}\\Sigma^{m}_{i=1}(h_{\\theta}(x^{(i)}) - y^{(i)})x^{(i)}_{j}$\n",
    "\n",
    "\n",
    "How did we get shorter? Well it's because we've packed more logic into our gradient descent update loop. j is going to assume the values between 0 and n, which means that it's going to update all thetas for all of our feature vectors.\n",
    "\n",
    "\n",
    "If we were to build it out for a problem that took in two inputs for example it would be as follows.\n",
    "\n",
    "$\\theta_{0} = \\theta_{0} - \\alpha\\frac{1}{m}\\Sigma^{m}_{i=1}(h_{\\theta}(x^{(i)}) - y^{(i)})x^{(i)}_{0}$\n",
    "\n",
    "$\\theta_{1} = \\theta_{1} - \\alpha\\frac{1}{m}\\Sigma^{m}_{i=1}(h_{\\theta}(x^{(i)}) - y^{(i)})x^{(i)}_{1}$\n",
    "\n",
    "$\\theta_{2} = \\theta_{2} - \\alpha\\frac{1}{m}\\Sigma^{m}_{i=1}(h_{\\theta}(x^{(i)}) - y^{(i)})x^{(i)}_{2}$\n",
    "\n",
    "This is the new update cycle that would be repeated until convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f02ac1f",
   "metadata": {},
   "source": [
    "This is not a large step up from what we are used to. It simply has to update more thetas since there are more values to account for in our new Regression Model. These thetas are going to be attached to their respective feature."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
