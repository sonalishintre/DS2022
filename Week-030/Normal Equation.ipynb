{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abde4df7",
   "metadata": {},
   "source": [
    "Gradient Descent using Loss isn't our only way to calculate the predictor line(or hyperplane). We can use this as a general purpose way to predict some continous output.\n",
    "\n",
    "This isn't however, the only way to predict continuous output, there are a variety of ways to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1570d5",
   "metadata": {},
   "source": [
    "# Normal Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3d2cc6",
   "metadata": {},
   "source": [
    "The normal equation is another commonly used way of calculating the coefficients that minimize loss. The normal equation tries to solve the same problem as MSE LOSS, it aims to minimize the Sum of Residual Squares. This is a closed form solution however, and let's try to understand what it is doing.\n",
    "    \n",
    "    \n",
    "    \n",
    "$\\theta = (X^{\\intercal}X)^{-1}X^{\\intercal}y$\n",
    "\n",
    "\n",
    "What we're doing here in a very simple way is computing the values for the theta vector that will cause the partial derivatives to equal zero. Why would we want all of the partial derivatives to equal zero? Well that's because if we think about our loss function, when all of the partial derivatives are equal to zero, that means that we found the global minimum. Let's look at it from a 1D perspsective. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3cba34",
   "metadata": {},
   "source": [
    "# If we have this closed form solution for finding the least squares, then why do we bother with Stochastic Gradient Descent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b78428e",
   "metadata": {},
   "source": [
    "The answer is because of time complexity. \n",
    "\n",
    "The pros and cons list are as follow.\n",
    "\n",
    "\n",
    "    Gradient Descent\n",
    "\n",
    "    Needs to find an optimal alpha.\n",
    "    Needs many iterations.\n",
    "    Works well even with a large dataset\n",
    "\n",
    "\n",
    "    Normal Equation\n",
    "\n",
    "    No need to choose alpha\n",
    "    No iteration\n",
    "    Computationally Expensive\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
